{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "In this tutorial, we will show you how to train an actual CNN model, albeit small. We will be using the good ol' MNIST dataset and the LeNet model, with a slight change that the sigmoid activations are replaced with ReLUs.\n",
    "\n",
    "We will use the model helper - that helps us to deal with parameter initializations naturally.\n",
    "\n",
    "First, let's import the necessities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "from caffe2.python import core, model_helper, net_drawer, workspace, visualize, brew\n",
    "\n",
    "# If you would like to see some really detailed initializations,\n",
    "# you can change --caffe2_log_level=0 to --caffe2_log_level=-1\n",
    "core.GlobalInit(['caffe2', '--caffe2_log_level=0'])\n",
    "print(\"Necessities imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will track statistics during the training time and store these on disk in a local folder. We need to set up a data folder for the data and a root folder for the stats. You should already have these folders, and in the data folder the MNIST dataset should be setup as a leveldb database for both the training set and the test set for this tutorial. \n",
    "\n",
    "If these folders are missing then you will need to [download the MNIST dataset](Models_and_Datasets.ipynb), g/unzip the dataset and labels, then find the binaries in `/caffe2/build/caffe2/binaries/` or in `/usr/local/bin/` and run the following, however the code block below will attempt to do this for you, so try that first.\n",
    "\n",
    "```\n",
    "./make_mnist_db --channel_first --db leveldb --image_file ~/Downloads/train-images-idx3-ubyte --label_file ~/Downloads/train-labels-idx1-ubyte --output_file ~/caffe2/caffe2/python/tutorials/tutorial_data/mnist/mnist-train-nchw-leveldb\n",
    "\n",
    "./make_mnist_db --channel_first --db leveldb --image_file ~/Downloads/t10k-images-idx3-ubyte --label_file ~/Downloads/t10k-labels-idx1-ubyte --output_file ~/caffe2/caffe2/python/tutorials/tutorial_data/mnist/mnist-test-nchw-leveldb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This section preps your image and test set in a leveldb\n",
    "current_folder = os.path.join(os.path.expanduser('~'), 'caffe2_notebooks')\n",
    "\n",
    "data_folder = os.path.join(current_folder, 'tutorial_data', 'mnist')\n",
    "root_folder = os.path.join(current_folder, 'tutorial_files', 'tutorial_mnist')\n",
    "image_file_train = os.path.join(data_folder, \"train-images-idx3-ubyte\")\n",
    "label_file_train = os.path.join(data_folder, \"train-labels-idx1-ubyte\")\n",
    "image_file_test = os.path.join(data_folder, \"t10k-images-idx3-ubyte\")\n",
    "label_file_test = os.path.join(data_folder, \"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "# Get the dataset if it is missing\n",
    "def DownloadDataset(url, path):\n",
    "    import requests, zipfile, StringIO\n",
    "    print \"Downloading... \", url, \" to \", path\n",
    "    r = requests.get(url, stream=True)\n",
    "    z = zipfile.ZipFile(StringIO.StringIO(r.content))\n",
    "    z.extractall(path)\n",
    "\n",
    "def GenerateDB(image, label, name):\n",
    "    name = os.path.join(data_folder, name)\n",
    "    print 'DB: ', name\n",
    "    if not os.path.exists(name):\n",
    "        syscall = \"/usr/local/bin/make_mnist_db --channel_first --db leveldb --image_file \" + image + \" --label_file \" + label + \" --output_file \" + name\n",
    "        # print \"Creating database with: \", syscall\n",
    "        os.system(syscall)\n",
    "    else:\n",
    "        print \"Database exists already. Delete the folder if you have issues/corrupted DB, then rerun this.\"\n",
    "        if os.path.exists(os.path.join(name, \"LOCK\")):\n",
    "            # print \"Deleting the pre-existing lock file\"\n",
    "            os.remove(os.path.join(name, \"LOCK\"))\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "if not os.path.exists(label_file_train):\n",
    "    DownloadDataset(\"https://download.caffe2.ai/datasets/mnist/mnist.zip\", data_folder)\n",
    "    \n",
    "if os.path.exists(root_folder):\n",
    "    print(\"Looks like you ran this before, so we need to cleanup those old files...\")\n",
    "    shutil.rmtree(root_folder)\n",
    "    \n",
    "os.makedirs(root_folder)\n",
    "workspace.ResetWorkspace(root_folder)\n",
    "\n",
    "# (Re)generate the leveldb database (known to get corrupted...) \n",
    "GenerateDB(image_file_train, label_file_train, \"mnist-train-nchw-leveldb\")\n",
    "GenerateDB(image_file_test, label_file_test, \"mnist-test-nchw-leveldb\")\n",
    "\n",
    "    \n",
    "print(\"training data folder:\" + data_folder)\n",
    "print(\"workspace root folder:\" + root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `ModelHelper` class to represent our main model and using `brew` module and `Operators` to build our model. `brew` module has a set of wrapper functions that automatically separates the parameter intialization and the actual computation into two networks. Under the hood, a `ModelHelper` object has two underlying nets, `param_init_net` and `net`, that keeps record of the initialization network and the main network respectively.\n",
    "\n",
    "For the sake of modularity, we will separate the model to multiple different parts:\n",
    "\n",
    "    (1) The data input part (AddInput function)\n",
    "    (2) The main computation part (AddLeNetModel function)\n",
    "    (3) The training part - adding gradient operators, update, etc. (AddTrainingOperators function)\n",
    "    (4) The bookkeeping part, where we just print out statistics for inspection. (AddBookkeepingOperators function)\n",
    "    \n",
    "`AddInput` will load the data from a DB. We store MNIST data in pixel values, so after batching this will give us data with shape `(batch_size, num_channels, width, height)`, in this case `[batch_size, 1, 28, 28]` of data type *uint8* and a label with shape `[batch_size]` of data type *int*.\n",
    "    \n",
    "Since we are going to do float computations, we will cast the data to the *float* data type.\n",
    "For better numerical stability, instead of representing data in [0, 255] range, we will scale them down to [0, 1].\n",
    "Note that we are doing in-place computation for this operator: we don't need the pre-scale data.\n",
    "Now, when computing the backward pass, we will not need the gradient computation for the backward pass. `StopGradient` does exactly that: in the forward pass it does nothing and in the backward pass all it does is to tell the gradient generator \"the gradient does not need to pass through me\".\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddInput(model, batch_size, db, db_type):\n",
    "    # load the data\n",
    "    data_uint8, label = model.TensorProtosDBInput(\n",
    "        [], [\"data_uint8\", \"label\"], batch_size=batch_size,\n",
    "        db=db, db_type=db_type)\n",
    "    # cast the data to float\n",
    "    data = model.Cast(data_uint8, \"data\", to=core.DataType.FLOAT)\n",
    "    # scale data from [0,255] down to [0,1]\n",
    "    data = model.Scale(data, data, scale=float(1./256))\n",
    "    # don't need the gradient for the backward pass\n",
    "    data = model.StopGradient(data, data)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need to take a look at the predictions coming out of the network at convert them into probabilities.\n",
    "\"What's the probability that this number we're looking at is a 5\", or \"is this a 7\", and so forth. \n",
    "\n",
    "The results will be conformed into a range between 0 and 1 such that the closer you are to 1 the more likely the number matches the prediction. The process that we can use to do this is available in LeNet and will provide us the *softmax* prediction. The `AddLeNetModel` function below will output the `softmax`. However, in this case, it does much more than the softmax - it is the computed model with its convoluted layers, as well as the softmax.\n",
    "\n",
    "[An explanation of kernels in image processing](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29) might be useful for more info on why `kernel=5` is used in the convolutional layers below. `dim_in` is the number of input channels and `dim_out` is the number of output channels. \n",
    "\n",
    "As you can see below `conv1` has 1 channel coming in (`dim_in`) and 20 going out (`dim_out`), whereas `conv2` has 20 going in and 50 going out and `fc3` has 50 going in and 500 going out. The images are transformed to smaller sizes along each convolution.d\n",
    "\n",
    "TODO: include image of the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddLeNetModel(model, data):\n",
    "    '''\n",
    "    This part is the standard LeNet model: from data to the softmax prediction.\n",
    "    \n",
    "    For each convolutional layer we specify dim_in - number of input channels\n",
    "    and dim_out - number or output channels. Also each Conv and MaxPool layer changes the\n",
    "    image size. For example, kernel of size 5 reduces each side of an image by 4.\n",
    "\n",
    "    While when we have kernel and stride sizes equal 2 in a MaxPool layer, it divides\n",
    "    each side in half.\n",
    "    '''\n",
    "    # Image size: 28 x 28 -> 24 x 24\n",
    "    conv1 = brew.conv(model, data, 'conv1', dim_in=1, dim_out=20, kernel=5)\n",
    "    # Image size: 24 x 24 -> 12 x 12\n",
    "    pool1 = brew.max_pool(model, conv1, 'pool1', kernel=2, stride=2)\n",
    "    # Image size: 12 x 12 -> 8 x 8\n",
    "    conv2 = brew.conv(model, pool1, 'conv2', dim_in=20, dim_out=50, kernel=5)\n",
    "    # Image size: 8 x 8 -> 4 x 4\n",
    "    pool2 = brew.max_pool(model, conv2, 'pool2', kernel=2, stride=2)\n",
    "    # 50 * 4 * 4 stands for dim_out from previous layer multiplied by the image size\n",
    "    fc3 = brew.fc(model, pool2, 'fc3', dim_in=50 * 4 * 4, dim_out=500)\n",
    "    fc3 = brew.relu(model, fc3, fc3)\n",
    "    pred = brew.fc(model, fc3, 'pred', 500, 10)\n",
    "    softmax = brew.softmax(model, pred, 'softmax')\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AddAccuracy` function below adds an accuracy operator to the model. We will use this in the next function to keep track of the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddAccuracy(model, softmax, label):\n",
    "    \"\"\"Adds an accuracy op to the model\"\"\"\n",
    "    accuracy = brew.accuracy(model, [softmax, label], \"accuracy\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function, `AddTrainingOperators`, adds training operators to the model. \n",
    "\n",
    "In the first step, we apply an Operator, `LabelCrossEntropy`, that computes the cross entropy between the input and the label set. This operator is almost always used after getting a softmax and before computing the model's loss. It's going to take in the `[softmax, label]` array along with a label, `'xent'` for \"Cross Entropy\".\n",
    "\n",
    "    xent = model.LabelCrossEntropy([softmax, label], 'xent')\n",
    "    \n",
    "`AveragedLoss` will take in the cross entropy and return the average of the losses found in the cross entropy.\n",
    "\n",
    "    loss = model.AveragedLoss(xent, \"loss\")\n",
    "\n",
    "For bookkeeping purposes, we will also compute the accuracy of the model by invoking the AddAccuracy function like so:\n",
    "\n",
    "    AddAccuracy(model, softmax, label)\n",
    "\n",
    "The next line is the key part of the training model: we add all the gradient operators to the model. The gradient is computed with respect to the loss that we computed above.\n",
    "\n",
    "    model.AddGradientOperators([loss])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next handful of lines support a very simple stochastic gradient descent. \n",
    "--- TODO(jiayq): We are working on wrapping these SGD operations in a cleaner fashion, and we will update this when it is ready. For now, you can see how we basically express the SGD algorithms with basic operators. \n",
    "It isn't necessary to fully understand this part at the moment, but we'll walk you through the process anyway.\n",
    "\n",
    "`Iter` operator is a counter for the number of iterations we run in the training. We use `brew.iter` helper function to add it to model\n",
    "\n",
    "    ITER = brew.iter(model, \"iter\")\n",
    "\n",
    "We do a simple learning rate schedule where lr = base_lr * (t ^ gamma)\n",
    "Note that we are doing minimization, so the base_lr is negative so we are going the DOWNHILL direction. \n",
    "\n",
    "    LR = model.LearningRate(\n",
    "        ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999 )\n",
    "ONE is a constant value that is used in the gradient update. We only need to create it once, so it is explicitly placed in param_init_net.\n",
    "\n",
    "    ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n",
    "\n",
    "Now, for each parameter, we do the gradient updates. Note how we get the gradient of each parameter - `ModelHelper` keeps track of that. The update is a simple weighted sum: param = param + param_grad * LR\n",
    "\n",
    "    for param in model.params:\n",
    "        param_grad = model.param_to_grad[param]\n",
    "        model.WeightedSum([param, ONE, param_grad, LR], param)        \n",
    "        \n",
    "We will need to checkpoint the parameters of the model periodically. This is achieved via the Checkpoint operator. It also takes in a parameter \"every\" so that we don't checkpoint way too often. In this case, we will say let's checkpoint every 20 iterations, which should probably be fine.\n",
    "\n",
    "    model.Checkpoint([ITER] + model.params, [],\n",
    "                   db=\"mnist_lenet_checkpoint_%05d.leveldb\",\n",
    "                   db_type=\"leveldb\", every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddTrainingOperators(model, softmax, label):\n",
    "    \"\"\"Adds training operators to the model.\"\"\"\n",
    "    xent = model.LabelCrossEntropy([softmax, label], 'xent')\n",
    "    # compute the expected loss\n",
    "    loss = model.AveragedLoss(xent, \"loss\")\n",
    "    # track the accuracy of the model\n",
    "    AddAccuracy(model, softmax, label)\n",
    "    # use the average loss we just computed to add gradient operators to the model\n",
    "    model.AddGradientOperators([loss])\n",
    "    # do a simple stochastic gradient descent\n",
    "    ITER = brew.iter(model, \"iter\")\n",
    "    # set the learning rate schedule\n",
    "    LR = model.LearningRate(\n",
    "        ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999 )\n",
    "    # ONE is a constant value that is used in the gradient update. We only need\n",
    "    # to create it once, so it is explicitly placed in param_init_net.\n",
    "    ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n",
    "    # Now, for each parameter, we do the gradient updates.\n",
    "    for param in model.params:\n",
    "        # Note how we get the gradient of each parameter - ModelHelper keeps\n",
    "        # track of that.\n",
    "        param_grad = model.param_to_grad[param]\n",
    "        # The update is a simple weighted sum: param = param + param_grad * LR\n",
    "        model.WeightedSum([param, ONE, param_grad, LR], param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `AddBookkeepingOperations`, adds a few bookkeeping operators that we can inspect later. These operators do not affect the training procedure: they only collect statistics and prints them to file or to logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddBookkeepingOperators(model):\n",
    "    \"\"\"This adds a few bookkeeping operators that we can inspect later.\n",
    "    \n",
    "    These operators do not affect the training procedure: they only collect\n",
    "    statistics and prints them to file or to logs.\n",
    "    \"\"\"    \n",
    "    # Print basically prints out the content of the blob. to_file=1 routes the\n",
    "    # printed output to a file. The file is going to be stored under\n",
    "    #     root_folder/[blob name]\n",
    "    model.Print('accuracy', [], to_file=1)\n",
    "    model.Print('loss', [], to_file=1)\n",
    "    # Summarizes the parameters. Different from Print, Summarize gives some\n",
    "    # statistics of the parameter, such as mean, std, min and max.\n",
    "    for param in model.params:\n",
    "        model.Summarize(param, [], to_file=1)\n",
    "        model.Summarize(model.param_to_grad[param], [], to_file=1)\n",
    "    # Now, if we really want to be verbose, we can summarize EVERY blob\n",
    "    # that the model produces; it is probably not a good idea, because that\n",
    "    # is going to take time - summarization do not come for free. For this\n",
    "    # demo, we will only show how to summarize the parameters and their\n",
    "    # gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's actually create the models for training and testing. If you are seeing WARNING messages below, don't be alarmed. The functions we established earlier are now going to be executed. Remember the four steps that we're doing:\n",
    "\n",
    "    (1) data input  \n",
    "    (2) main computation\n",
    "    (3) training \n",
    "    (4) bookkeeping\n",
    "    \n",
    "Before we can do the data input though we need to define our training model. We will basically need every piece of the components we defined above. In this example, we're using NCHW storage order on the mnist_train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg_scope = {\"order\": \"NCHW\"}\n",
    "train_model = model_helper.ModelHelper(name=\"mnist_train\", arg_scope=arg_scope)\n",
    "data, label = AddInput(\n",
    "    train_model, batch_size=64,\n",
    "    db=os.path.join(data_folder, 'mnist-train-nchw-leveldb'),\n",
    "    db_type='leveldb')\n",
    "softmax = AddLeNetModel(train_model, data)\n",
    "AddTrainingOperators(train_model, softmax, label)\n",
    "AddBookkeepingOperators(train_model)\n",
    "\n",
    "# Testing model. We will set the batch size to 100, so that the testing\n",
    "# pass is 100 iterations (10,000 images in total).\n",
    "# For the testing model, we need the data input part, the main LeNetModel\n",
    "# part, and an accuracy part. Note that init_params is set False because\n",
    "# we will be using the parameters obtained from the train model.\n",
    "test_model = model_helper.ModelHelper(\n",
    "    name=\"mnist_test\", arg_scope=arg_scope, init_params=False)\n",
    "data, label = AddInput(\n",
    "    test_model, batch_size=100,\n",
    "    db=os.path.join(data_folder, 'mnist-test-nchw-leveldb'),\n",
    "    db_type='leveldb')\n",
    "softmax = AddLeNetModel(test_model, data)\n",
    "AddAccuracy(test_model, softmax, label)\n",
    "\n",
    "# Deployment model. We simply need the main LeNetModel part.\n",
    "deploy_model = model_helper.ModelHelper(\n",
    "    name=\"mnist_deploy\", arg_scope=arg_scope, init_params=False)\n",
    "AddLeNetModel(deploy_model, \"data\")\n",
    "# You may wonder what happens with the param_init_net part of the deploy_model.\n",
    "# No, we will not use them, since during deployment time we will not randomly\n",
    "# initialize the parameters, but load the parameters from the db.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let's take a look what the training and deploy models look like, using the simple graph visualization tool that Caffe2 has. If the following command fails for you, it might be because that the machine you run on does not have graphviz installed. You can usually install that by:\n",
    "\n",
    "```sudo yum install graphviz```\n",
    "\n",
    "If the graph looks too small, right click and open the image in a new tab for better inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "graph = net_drawer.GetPydotGraph(train_model.net.Proto().op, \"mnist\", rankdir=\"LR\")\n",
    "display.Image(graph.create_png(), width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the graph above shows everything that is happening in the training phase: the white nodes are the blobs, and the green rectangular nodes are the operators being run. You may have noticed the massive parallel lines like train tracks: these are dependencies from the blobs generated in the forward pass to their backward operators.\n",
    "\n",
    "Let's display the graph in a more minimal way by showing only the necessary dependencies and only showing the operators. If you read carefully, you can see that the left half of the graph is the forward pass, the right half of the graph is the backward pass, and on the very right there are a set of parameter update and summarization operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = net_drawer.GetPydotGraphMinimal(\n",
    "    train_model.net.Proto().op, \"mnist\", rankdir=\"LR\", minimal_dependency=True)\n",
    "display.Image(graph.create_png(), width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run the network, one way is to directly run it from Python. Remember as we are running the network, we can periodically pull blobs from the network - Let's first show how we do this.\n",
    "\n",
    "Before, that, let's re-iterate the fact that, the ModelHelper class has not executed anything yet. All it does is to *declare* the network, which is basically creating the protocol buffers. For example, we will show a portion of the serialized protocol buffer for the training models' param init net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(str(train_model.param_init_net.Proto())[:400] + '\\n...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also dump all the protocol buffers to disk so you can easily inspect them. As you may have noticed, these protocol buffers are much like the old good caffe's network definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(root_folder, \"train_net.pbtxt\"), 'w') as fid:\n",
    "    fid.write(str(train_model.net.Proto()))\n",
    "with open(os.path.join(root_folder, \"train_init_net.pbtxt\"), 'w') as fid:\n",
    "    fid.write(str(train_model.param_init_net.Proto()))\n",
    "with open(os.path.join(root_folder, \"test_net.pbtxt\"), 'w') as fid:\n",
    "    fid.write(str(test_model.net.Proto()))\n",
    "with open(os.path.join(root_folder, \"test_init_net.pbtxt\"), 'w') as fid:\n",
    "    fid.write(str(test_model.param_init_net.Proto()))\n",
    "with open(os.path.join(root_folder, \"deploy_net.pbtxt\"), 'w') as fid:\n",
    "    fid.write(str(deploy_model.net.Proto()))\n",
    "print(\"Protocol buffers files have been created in your root folder: \" + root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run the training procedure. We will drive all the computation in Python here, however you can also write a plan out to disk so that you can completely train stuff in C++.  We'll leave discussion on that route for another tutorial.\n",
    "\n",
    "Please note that this process will take a while to run. Keep an eye on the asterisk (In [\\*]) or other IPython indicators that the code block is still running.\n",
    "\n",
    "First we must initialize the network with:\n",
    "\n",
    "    workspace.RunNetOnce(train_model.param_init_net)\n",
    "    \n",
    "Since we are going to run the main network multiple times, we first create the network which puts the actual network generated from the protobuf into the workspace.\n",
    "\n",
    "    workspace.CreateNet(train_model.net)\n",
    "\n",
    "We will set the number of iterations that we'll run the network to 200 and create two numpy arrays to record the accuracy and loss for each iteration.\n",
    "\n",
    "    total_iters = 200\n",
    "    accuracy = np.zeros(total_iters)\n",
    "    loss = np.zeros(total_iters)\n",
    "\n",
    "With the network and tracking of accuracy and loss setup we can now loop the 200 interations calling `workspace.RunNet` and passing the name of the network `train_model.net.Proto().name`. On each iteration we calculate the accuracy and loss with `workspace.FetchBlob('accuracy')` and `workspace.FetchBlob('loss')`.\n",
    "\n",
    "    for i in range(total_iters):\n",
    "        workspace.RunNet(train_model.net.Proto().name)\n",
    "        accuracy[i] = workspace.FetchBlob('accuracy')\n",
    "        loss[i] = workspace.FetchBlob('loss')\n",
    "\n",
    "Finally, we can plot the results using `pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The parameter initialization network only needs to be run once.\n",
    "workspace.RunNetOnce(train_model.param_init_net)\n",
    "# creating the network\n",
    "workspace.CreateNet(train_model.net, overwrite=True)\n",
    "# set the number of iterations and track the accuracy & loss\n",
    "total_iters = 200\n",
    "accuracy = np.zeros(total_iters)\n",
    "loss = np.zeros(total_iters)\n",
    "# Now, we will manually run the network for 200 iterations. \n",
    "for i in range(total_iters):\n",
    "    workspace.RunNet(train_model.net)\n",
    "    accuracy[i] = workspace.FetchBlob('accuracy')\n",
    "    loss[i] = workspace.FetchBlob('loss')\n",
    "# After the execution is done, let's plot the values.\n",
    "pyplot.plot(loss, 'b')\n",
    "pyplot.plot(accuracy, 'r')\n",
    "pyplot.legend(('Loss', 'Accuracy'), loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sample some of the data and predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at some of the data.\n",
    "pyplot.figure()\n",
    "data = workspace.FetchBlob('data')\n",
    "_ = visualize.NCHW.ShowMultiple(data)\n",
    "pyplot.figure()\n",
    "softmax = workspace.FetchBlob('softmax')\n",
    "_ = pyplot.plot(softmax[0], 'ro')\n",
    "pyplot.title('Prediction for the first image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutions for this mini-batch\n",
    "pyplot.figure()\n",
    "conv = workspace.FetchBlob('conv1')\n",
    "shape = list(conv.shape)\n",
    "shape[1] = 1\n",
    "# We can look into any channel. This of it as a feature model learned\n",
    "conv = conv[:,15,:,:].reshape(shape)\n",
    "\n",
    "_ = visualize.NCHW.ShowMultiple(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we created the test net? We will run the test pass and report the test accuracy here. Note that although test_model will be using the parameters obtained from train_model, test_model.param_init_net must still be run to initialize the input data.\n",
    "In this run, we only need to track the accuracy and we're also only going to run 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run a test pass on the test net\n",
    "workspace.RunNetOnce(test_model.param_init_net)\n",
    "workspace.CreateNet(test_model.net, overwrite=True)\n",
    "test_accuracy = np.zeros(100)\n",
    "for i in range(100):\n",
    "    workspace.RunNet(test_model.net.Proto().name)\n",
    "    test_accuracy[i] = workspace.FetchBlob('accuracy')\n",
    "# After the execution is done, let's plot the values.\n",
    "pyplot.plot(test_accuracy, 'r')\n",
    "pyplot.title('Acuracy over test batches.')\n",
    "print('test_accuracy: %f' % test_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the MNIST tutorial. We hope this tutorial highlighted some of Caffe2's features and how easy it is to create a simple CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
